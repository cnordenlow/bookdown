# R

This is chapter 1.

## Import & Export 

### Import


**Import fast using httpcashe**
  
*Improving efficiency in importing*


```{r, echo=T, eval=F}
get_data <- function(url) {
  httpcache::GET(url) %>%
    httr::content()
}

url_jobless_claims="https://oui.doleta.gov/unemploy/csv/ar539.csv"

data_jobless_claims <- get_data(url_jobless_claims)
```


**Import all files in a folder**
  
*Import all files in a folder. In the example below files are named "2020-05-05 Saldo". Import and create a table where the date of the filename is used in a column. Change name for column 1 and 2.*

*Map has similiar functionality to lapply. When you add \_dfr it will generate <span style="color:red">data.frames</span> and that these is merged.*


```{r, echo=T, eval=F}
parse_date <- function(x) as.Date(gsub( ".*(\\d{4}-\\d{2}-\\d{2}).*", "\\1", x))
dir_loc <- 'X:\\Likviditet\\RIX-filer\\Saldo'
rix_saldo <- dir(dir_loc, full.names = T) %>%
  map_dfr(~{
    read.csv2(.x, skip = 1, header = F) %>%
      mutate(date = as.Date(parse_date(basename(.x))))
  })
colnames(rix_saldo)[colnames(rix_saldo) == 'V1'] <- 'Participant'
colnames(rix_saldo)[colnames(rix_saldo) == 'V2'] <- 'Saldo'
```




**Import excel from web**
  
*Import excel from web by downloading it temp*


```{r, echo=T, eval=F}
library(readxl)
url_data_gdp <- ("https://www.bea.gov/system/files/2020-04/qgdpstate0420.xlsx")
download.file(url=url_data_gdp, destfile="localcopy.xlsx", mode="wb")

#Table 1: Percent Change in Real Gross Domestic Product (GDP) by State and state
table1 <- read_excel('localcopy.xlsx', sheet = 1, skip =4, col_names = FALSE)

```



### Export


**Export to txt file**
  

```{r, echo=T, eval=F}

write.table(table_for_report, "...\\Operations\\LikvProg\\likvprog_history.txt", sep="\t")

```

**Get table to paste into excel**

```{r, echo=T, eval=F}

  
write.excel <- function(x,row.names=FALSE,col.names=TRUE,...) {
  write.table(df,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}

write.excel(my.df)
```

## Tidy & Transform

### Cleaning


**Cleaning some data**
  
*Gather, Spread, Separate, Unite*

 
 ```{r, echo=T, eval=F}
 
library(tidyr)

#Create a messy dataset
messy <- data.frame(
  country = c("A", "B", "C"),
  q1_2017 = c(0.03, 0.05, 0.01),
  q2_2017 = c(0.05, 0.07, 0.02),
  q3_2017 = c(0.04, 0.05, 0.01),
  q4_2017 = c(0.03, 0.02, 0.04))
messy

#Reshape the data. in this function we create two new variables instead of the one in the original dataset.
tidier <- messy%>%
  gather(quarter, growth, q1_2017:q4_2017)
tidier

#Spread
#the spread function does the opposite of gather.
#Reshape the tidier dataset back to messy.

messy_1 <- tidier %>%
  spread(quarter, growth)
messy_1

#Separate
#Separate splits a column into two according to a separator. This function is helpful in some situations where the variable is a date, i.e. separate year and month.

separate_tidier <- tidier %>%
  separate(quarter, c("Qrt", "year"), sep ="_")
head(separate_tidier)

#Unite
#Unite concatenates two columns into one.

unit_tidier <- separate_tidier%>%
  unite(Quarter, Qrt, year, sep = "_")
head(unit_tidier)
  
 ```
 
 
 

### Expand

**Expand table**
  
*One example with expanding to all alternatives. Another to fill in gaps.*

```{r, echo=T, eval=F}
library(tidyverse)
library(dplyr)
##Expand all alternatives
a <- c(1:10)
b <- c(1:10)
c <- c(1:10)
df <- tibble(a,b,c)
all_combinations <- expand(df, a,b,c) 
#Expand by missing Date
df <- tibble(
  year   = c(2010, 2010, 2010, 2010, 2012, 2012, 2012),
  qtr    = c(   1,    2,    3,    4,    1,    2,    3),
  return = rnorm(7)
)
df %>% expand(year, qtr)
df %>% expand(year = 2010:2012, qtr)
df %>% complete(year = full_seq(year, 1), qtr)
```


### Join and Merge

**Join tables**
  
*Different ways to join tables.*

 
 ```{r, echo=T, eval=F}
 library(dplyr)

df_primary <- tribble(
~ID,~y,
"A", 5,
"B", 5,
"C", 8,
"D", 0,
"E", 9)

df_secondary <- tribble(
  ~ID,~y,
  "A", 30,
  "B", 21,
  "C", 22,
  "D", 25,
  "F", 29)
  
#Most common way to merge two datasets is to uset the left_join() function.
left_join_ <- left_join(df_primary, df_secondary, by ='ID')

#The right_join works like the left one.
right_join_ <- right_join(df_primary, df_secondary, by = 'ID')


#When we are sure that two datasets won´t match, we can consider to return only rows existing in both datasets. 
#This is legit when we need a clean dataset or when we dont want to impute missing values with the mean or median.
inner_join_ <- inner_join(df_primary, df_secondary, by ='ID')

# Full_join keeps all observations and replace missing values with NA.
full_join_ <- full_join(df_primary, df_secondary, by = 'ID')

  
 ```
 
 **Join tables on multiple conditions**
 
 *Join Tables on multiple conditions*

 
 ```{r, echo=T, eval=F}
 library(dplyr)

df_primary <- tribble(
  ~ID, ~year, ~items,
  "A", 2015,3,
  "A", 2016,7,
  "A", 2017,6,
  "B", 2015,4,
  "B", 2016,8,
  "B", 2017,7,
  "C", 2015,4,
  "C", 2016,6,
  "C", 2017,6)
df_secondary <- tribble(
  ~ID, ~year, ~prices,
  "A", 2015,9,
  "A", 2016,8,
  "A", 2017,12,
  "B", 2015,13,
  "B", 2016,14,
  "B", 2017,6,
  "C", 2015,15,
  "C", 2016,15,
  "C", 2017,13)
left_join(df_primary, df_secondary, by = c('ID', 'year'))

  
 ```
 
 **Merge Data Frames**
 
 *Merge Data Frames in R: Full and partial match*
  
 ```{r, echo=T, eval=F}
 producers <- data.frame(   
  surname =  c("Spielberg","Scorsese","Hitchcock","Tarantino","Polanski"),    
  nationality = c("US","US","UK","US","Poland"),    
  stringsAsFactors=FALSE)

# Create destination dataframe
movies <- data.frame(    
  surname = c("Spielberg",
              "Scorsese",
              "Hitchcock",
              "Hitchcock",
              "Spielberg",
              "Tarantino",
              "Polanski"),    
  title = c("Super 8",
            "Taxi Driver",
            "Psycho",
            "North by Northwest",
            "Catch Me If You Can",
            "Reservoir Dogs","Chinatown"),                
  stringsAsFactors=FALSE)

m1 <- merge(producers, movies, by.x = "surname")
m1

# Change name of ` movies ` dataframe
colnames(movies)[colnames(movies) == 'surname'] <- 'name'

# Merge with different key value
m2 <- merge(producers, movies, by.x = "surname", by.y = "name")

##Partial match
# Create a new producer
add_producer <-  c('Lucas', 'US')
#  Append it to the ` producer` dataframe
producers <- rbind(producers, add_producer)
# Use a partial merge 
m3 <-merge(producers, movies, by.x = "surname", by.y = "name", all.x = TRUE)
m3


```
 
 
 
### Transforming data with Apply etc

**apply(), lapply(), sapply(), tapply()**
  
*apply()*


```{r, echo=T, eval=F}
library(dplyr)
m1 <- matrix(c<-(1:10), nrow=5,ncol=6)
m1

#Sums columns
a_m1 <- apply(m1,2,sum)
a_m1

#Sums rows
a_m1 <- apply(m1,1,sum)
a_m1
```

*lapply()*

```{r, echo=T, eval=F}
library(dplyr)
movies <- c("spyderman", "batman", "vertigo", "chinatown")
movies_lower <- lapply(movies, tolower)
str(movies_lower)

#if we like to convert the list into a vector we can use unlist()
movies_lower <- unlist(lapply(movies, tolower))
str(movies_lower)


```
 
*sapply()*


```{r, echo=T, eval=F}
#sapply() function does the same jobs as lapply() function but returns a vectorÄ

library(dplyr)
dt <- cars
lmn_cars <- lapply(dt, min)
smn_cars <- sapply(dt,min)
lmn_cars

smn_cars


lmxcars <- lapply(dt,max)
smxcars <- sapply(dt,max)

lmxcars
smxcars

#lets create a function names avg to compute the average of the minimun and maximun of the vector.

avg <- function(x){
  (min(x) + max(x))/2
}
fcars <- sapply(dt, avg)
fcars

#sapply() function is more efficient than lapply() in the output returned because sapply() store values directly into a vector.


#it is possible to use lapply or sapply interchangeable to slice a data frame.
#lets compute a function that takes a vector of numerical values and returns a vector that only contains the values that are strictly above the average.

below_ave <- function(x){
  ave <- mean(x)
  return(x[x>ave])
}

dt_s <- sapply(dt, below_ave)
dt_l <- lapply(dt, below_ave)
identical(dt_s, dt_l)

``` 
 


*tapply()*


 
```{r, echo=T, eval=F}
#The function tapply() computes a measure (mean, median, min, max) or a function for each factor variable in a vector

library(dplyr)
data(iris)
tapply(iris$Sepal.Width, iris$Species, median)
``` 



### Tally-function

**Tally()**

*Tally is a useful wrapper for summarise with grouping conditions. In the example below we have a data set with countries. For US, there are no aggregate number, so we need to summarize each state.*

```{r, echo=T, eval=F}
library(tidyr)
library(dplyr)
df <- tibble::tribble(
  ~country, ~state, ~t1, ~t2,
  "SE", NA, 1,2,
  "US", "A", 10,20,
  "US", "B", 11,21,
)

df%>%
  tidyr::gather(date, value, -country, -state)%>%
  group_by(country, date) %>%
  tally(value)
``` 

 
 

### Useful functions / expressions

**Gsub**
  
*gsub() replaces all matches of a string.*

```{r, echo=T, eval=F}
x <- "Old City"
gsub("Old", "New", x)

#case insensitive
gsub("old", "New", x, ignore.case=T)

#Vector replacement
y <- c("Stockholm City", "Uppsala City", "Malmö")
gsub(" City","",y)

 
 ``` 
 
 **rnorm**
  
*Generate number from a normal distribution.*
```{r, echo=T, eval=F}
 rnorm(4)
#> [1] -2.3308287 -0.9073857 -0.7638332 -0.2193786

# Use a different mean and standard deviation
rnorm(4, mean=50, sd=10)
#> [1] 59.20927 40.12440 44.58840 41.97056

# To check that the distribution looks right, make a histogram of the numbers
x <- rnorm(400, mean=50, sd=10)
hist(x)
 ``` 
  
 **Create table**
  
*Create a table with combination fixed and random number * 
 ```{r, echo=T, eval=F}
 
 library(tidyverse)
 
 df <- tibble(
  value = seq(10,90,1),
  rand = seq(10,90,1) +runif(81, min=-10, max=15)
) 
``` 


## Visualize


### Ggplots

**geom_line with geom_ribbon**
  
*geom_line with geom_ribbon for pos / neg numbers*
 
```{r, echo=T, eval=F}
library(ggplot2)

df <- tibble(
  value = seq(1,50,1),
  rand = seq(1,50,1) +runif(50, min=-10, max=15)
)%>%
  mutate(diff = rand - value)


exposure_graph <- ggplot(df, aes(x=value,y=rand)) +
  geom_ribbon(aes(ymin=pmin(df$diff,0), ymax=0), fill="red", col="black", alpha=0.5) +
  geom_ribbon(aes(ymin=0, ymax=pmax(df$diff,0)), fill="blue", col="black", alpha=0.5) +
  geom_line(aes(y=0))
``` 



### Different tables

**Create table with kableExtra**
  
*Create table with different colors for pos / neg numbers*
 
```{r, echo=T, eval=F}
library(tidyverse)
library(kableExtra)


df <- tibble(
  type = c("gov_bond", "ssa", "ssa", "gov_bond","ssa", "ssa", "gov_bond", "gov_bond", "gov_bond", "ssa"),
  maturity_bucket = as.integer(runif(10, min =1, max=6)),
  diff_bm = runif(10, min = -10, max = 10)
)

sum_type <- df %>%
  group_by(type, maturity_bucket)%>%
  summarise(
    diff_exposure = round(sum(diff_bm),1)
  )


## Create table with green for positive, red for negative
sum_table <- sum_type%>%
  
  mutate(
    diff_exposure = ifelse(diff_exposure < 0,
                      cell_spec(diff_exposure, "html", color = "red", bold = T),
                      cell_spec(diff_exposure, "html", color = "green", italic = T)))%>%
  
  kable("html", escape = F, format.args=list(big.mark=" ", scientific=F)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position= "right", fixed_thead = T)

sum_table
``` 




## Misc



### Moving average

**Create a moving average**
  
*Example of creating a moving average for dates.*

```{r, echo=T, eval=F}
library(tidyverse)
library(dplyr)
library(lubridate)
df <- tibble(
Date = seq.Date(Sys.Date()-19, Sys.Date(), by="day"),
indicator = c(rep(1,10),rep(2,10)),
value = rnorm(20)
)
df <- arrange(df, Date)
df  %>%
  group_by(indicator) %>% 
  mutate(MA_3m = slide_index_dbl(value, Date, mean, .before=lubridate::days(2), .after=0,.complete=T))
  
  #Use before or after = Inf if you like to get the calculation based on all values before or after.
  
```


### Date Formatting

**Different ways to format dates**
  
*Dates*



```{r, echo=T, eval=F}
as.Date("2/15/1986", format = "%m/%d/%Y")
  
```  


### Loops

#### For loop example
  
*Creates a non-linear function by using the polynomial of x between 1 and 4 and we store it in a list*

```{r, echo=T, eval=F}

#
# Create an empty list
list <- c()
# Create a for statement to populate the list
for (i in seq(1, 4, by=1)) {
  list[[i]] <- i*i
}
print(list)
```  

**For loop over a matrix**
  
*A matrix has 2-dimension, rows and columns. To iterate over a matrix, we have to define two for loop, namely one for the rows and another for the column.*

```{r, echo=T, eval=F}

# Create a matrix
mat <- matrix(data = seq(10, 20, by=1), nrow = 6, ncol =2)
# Create the loop with r and c to iterate over the matrix
for (r in 1:nrow(mat))   
  for (c in 1:ncol(mat))  
    print(paste("Row", r, "and column",c, "have values of", mat[r,c])) 

```  




#### For loop example
  
*Creates a non-linear function by using the polynomial of x between 1 and 4 and we store it in a list*

```{r, echo=T, eval=F}

#
# Create an empty list
list <- c()
# Create a for statement to populate the list
for (i in seq(1, 4, by=1)) {
  list[[i]] <- i*i
}
print(list)
```  

**Function for Right and Left**
  
*Functions for Right and Left.*

```{r, echo=T, eval=F}

library(dplyr)

right = function(text, num_char) {
  substr(text, nchar(text) - (num_char-1), nchar(text))
}
left = function(text, num_char) {
  substr(text, 1, num_char)
}

df <- tibble(
Date = seq.Date(Sys.Date()-19, Sys.Date(), by="day"),
indicator = c(rep(1,10),rep(2,10)),
value = rnorm(20)
)

left(df$value, 3)
right(df$Date, 3)


```  


## R Markdown 

  
### Render multiple reports

**Render multiple reports in different folders.**

*In the example below one report is created for each stated currency. Params = list(currency) is the key.*
  
```{r, echo=T, eval=F}
#Write in one R Script
#Remove old
file.remove("...xxx/report/Benchmark_R/Portfolio_report_GBP.html")
file.remove("...xxx/report/Benchmark_R/Portfolio_report_AUD.html")
file.remove("...xxx/report/Benchmark_R/Portfolio_report_EUR.html")
purrr::map(
  c("AUD", "EUR", "GBP"),
  ~ {
    res <- rmarkdown::render("...xxx\\report\\Benchmark_R\\R code\\Markdown BM.Rmd", output_file = sprintf("...xxx\\report\\Benchmark_R\\Portfolio_report_%s.html", .x), params = list(currency = .x))
    file.copy(res,  sprintf("...xxx\\report\\Benchmark_R\\Old_reports\\Portfolio_report_%1$s_%2$s.html", .x, Sys.Date()))
    file.copy(res,  sprintf("...xxx/report/Benchmark_R//Portfolio_report_%s.html", .x))
  } 
)
#Markdown Report header
---
#title: "Portfolio and benchmark report"
output: html_document
date: "`r Sys.Date()`"
author: christoffer.nordenlow@outlook.com
params:
  currency: "EUR"
  
title: "`r sprintf('Portfolio and benchmark report, %s', params$currency)`"  
---
  
``` 



## Web Scraping


### Scrape all sub page

**Scrape web page info and save in a table**
  
*Scrape all different sub web pages under a base page. In the below example there a number of sub pages under the base bage. R is scraping all different URL under the main page. Info in the tables under the sub pages are saved in a table. You will need to have HTTP_PROXY/HTTPS_PROXY as environment variables.*

   


```{r, echo=T, eval=F}
#https://cran.r-project.org/web/packages/rvest/rvest.pdf
require(rvest)
require(xml2)
require(tidyverse)
.base_url <- "https://www.riksbank.se"
doc <- read_html(file.path(.base_url, "sv/penningpolitik/penningpolitiska-instrument/kop-av-foretagscertifikat/special-terms-and-conditions/"))
urls <- doc %>%
  html_nodes("a") %>%
  html_attr("href")
urls <- urls[str_detect(urls, regex(".*/special-terms-and-conditions/.*bid-date.*$"))]
urls <- file.path(.base_url, urls)
names(urls) <- basename(urls)
doc_subpage <- read_html(urls[[1]])
df <- urls %>%
  map_dfr(~{
    doc_subpage %>%
      html_node("table") %>%
      html_table() %>%
      rename(key=X1, value=X2) %>%
      as_tibble()
  }, .id = "url")
  
  #It is possible to filter which files should be imported.
  #map(...) %>% filter(lubridate::year(date) == 2019)
  
  
```  



### Scrape PL table

**Scrape one table**


```{r, echo=T, eval=F}

library(rvest)

web_pl <- read_html("https://www.foxsports.com/soccer/stats?competition=1&season=20190&category=standard&sort=3")
tbls <- html_nodes(web_pl, "table")
head(tbls)


pl_stats <- web_pl %>%
  html_nodes("table") %>%
  # .[3:4] %>%
  html_table(fill = TRUE)%>%
  .[[1]]
  
```  


### Scrape all tables

**Scrape all tables, use one**


```{r, echo=T, eval=F}
##Web scrape US Data. Payroll
#http://bradleyboehmke.github.io/2015/12/scraping-html-tables.html

library(rvest)

web_bls <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

tbls <- html_nodes(web_bls, "table")  #extract all table nodes that exist on the page.

head(tbls)


#To parse the HTML, we use html_table. In this example it creates
table_bls <- web_bls %>%
  html_nodes("table") %>%
  .[3:4] %>% ##determines which tables. In this case, table 3 and 4.
  html_table(fill = TRUE)

str(table_bls)

#Extract table 2, non-farm
head(table_bls[[2]], 4)

# remove row 1 that includes part of the headings. Not neccessary here
#table_bls[[2]] <- table_bls[[2]][-1,]


table_bls2 <-table_bls[[2]]
```  


### Scrape title

**Scrape title**


```{r, echo=T, eval=F}

library(rvest)

lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")
lego_movie %>%
  html_node(xpath='//div[@class="originalTitle"]') %>%
  html_text() 

```  




